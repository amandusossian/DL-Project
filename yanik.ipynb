{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import enum\n",
    "import copy\n",
    "import connect4.connect4 as game\n",
    "from pympler import asizeof\n",
    "import deeplearning.buffer as buf\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import deeplearning.mlp as mlp\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = game.Connect4()\n",
    "randomPlayer1 = game.RandomPlayer()\n",
    "greedyPlayer2 = game.GreedyRandomPlayer()\n",
    "buffer = buf.ReplayBuffer(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO Before:  940 1060\n",
      "ELO After:  944.0 1056.0\n",
      "p1:  0.3371 p2:  0.6612 draw:  0.0017\n",
      "ELO Before:  944.0 944.0\n",
      "ELO After:  944.0 944.0\n",
      "p1:  0.5044 p2:  0.4912 draw:  0.0044\n",
      "ELO Before:  1056.0 1056.0\n",
      "ELO After:  1056.0 1056.0\n",
      "p1:  0.4818 p2:  0.5078 draw:  0.0104\n"
     ]
    }
   ],
   "source": [
    "gm = game.GameManager([randomPlayer1, greedyPlayer2])\n",
    "gm.play(10000, game.Connect4, buffer)\n",
    "gm.info()\n",
    "\n",
    "gm = game.GameManager([randomPlayer1, randomPlayer1])\n",
    "gm.play(5000, game.Connect4, buffer)\n",
    "gm.info()\n",
    "\n",
    "gm = game.GameManager([greedyPlayer2, greedyPlayer2])\n",
    "gm.play(5000, game.Connect4, buffer)\n",
    "gm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bf = mlp.BF(42, 7, 5000, 1, device).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 194.4607731103897 88 1.9422095\n",
      "200 194.05811738967896 174 1.9377781\n",
      "300 193.70407283306122 244 1.9341537\n",
      "400 193.39173138141632 398 1.9286159\n",
      "500 193.09932851791382 487 1.9249791\n",
      "600 192.86251831054688 586 1.9223009\n",
      "700 192.61722922325134 662 1.919573\n",
      "800 192.4673353433609 800 1.9151205\n",
      "900 192.2656056880951 878 1.9130182\n",
      "1000 192.0796616077423 916 1.9115233\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "def train_behavior_function(batch_size, model, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X, y = buffer.create_training_examples(batch_size)\n",
    "\n",
    "\n",
    "    X = torch.stack(X)\n",
    "\n",
    "\n",
    "    state = X[:,0:42]\n",
    "    d = X[:,42:42+1]\n",
    "    e = X[:,43:43+1]\n",
    "    command = torch.cat([d,e], dim=-1)\n",
    "    command2 = torch.cat([-d,e], dim=-1)\n",
    "    y = torch.FloatTensor((y)).to(device).long()\n",
    "    y_ = model(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    pred_loss = F.cross_entropy(y_, y)   \n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()\n",
    "\n",
    "def run_loop():\n",
    "    i = 0\n",
    "    cum_loss = 0\n",
    "    best_loss = 100\n",
    "    best_loss_i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        loss = train_behavior_function(1000, bf, optimizer)\n",
    "        if loss < best_loss - 0.001:\n",
    "            best_loss = loss\n",
    "            best_loss_i = i\n",
    "        if i - best_loss_i > 100:\n",
    "            print(\"early stopping\")\n",
    "            return\n",
    "        cum_loss += loss\n",
    "        if i % 100 == 0:\n",
    "            print(i, cum_loss, best_loss_i, best_loss)\n",
    "            cum_loss = 0\n",
    "run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33510917 0.08345829 0.0130851  0.5549511  0.00471462 0.00406335\n",
      " 0.00461842]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "x = bf.steps([s], -1, 900)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELO Before:  1028.0 970.0\n",
      "ELO After:  1032.0 966.0\n",
      "p1:  0.62 p2:  0.375 draw:  0.005\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    buf2 = buf.ReplayBuffer(20000)\n",
    "    gm = game.GameManager([bf, greedyPlayer2])\n",
    "    gm.play(1000, game.Connect4, buf2, 1060)\n",
    "    gm.info() # p1:  0.6 p2:  0.4 draw:  0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "          -0.0000,  1.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "          -1.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  1.0000,\n",
       "          -0.0000, -0.0000, -0.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,\n",
       "          -1.0000, -0.0000, -1.0000,  1.0600])],\n",
       " [0])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.create_training_examples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewer(a):\n",
    "    print(\"https://connect4.gamesolver.org/en/?pos=\" + \"\".join([str(x+1) for x in a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://connect4.gamesolver.org/en/?pos=1444454416111122222233353\n",
      "https://connect4.gamesolver.org/en/?pos=4444461741111122222523363\n",
      "https://connect4.gamesolver.org/en/?pos=474444141111122222233\n",
      "https://connect4.gamesolver.org/en/?pos=4445134442111112262223333355555666\n",
      "https://connect4.gamesolver.org/en/?pos=241447444711111627232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1028.0, 970.0],\n",
       " [1028.0, 970.0],\n",
       " [1028.0, 970.0],\n",
       " [1028.0, 970.0],\n",
       " [1028.0, 970.0]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[viewer(buf2.buffer[i]['actions']) for i in range(5)]\n",
    "[buf2.buffer[i]['elo'] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, d, i = env.step(bf.step(s.flatten(), 1))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, d, i = env.step([0, 0, 0, 1, 0, 0, 0])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.16947517\n",
      "100 0.2030002\n",
      "200 0.2503905\n",
      "300 0.29866624\n",
      "400 0.3451594\n",
      "500 0.38811266\n",
      "600 0.4266829\n",
      "700 0.4607666\n",
      "800 0.49070683\n",
      "900 0.5170649\n",
      "1000 0.5404376\n",
      "1100 0.56134963\n",
      "1200 0.5802656\n",
      "1300 0.5975504\n",
      "1400 0.6134952\n",
      "1500 0.62832177\n",
      "1600 0.64220375\n",
      "1700 0.65528154\n",
      "1800 0.6676629\n",
      "1900 0.67943215\n"
     ]
    }
   ],
   "source": [
    "# some fun visualizers\n",
    "# elo\n",
    "a = env.reset()\n",
    "for x in range(0, 2000, 100):\n",
    "    print(x, bf.steps([a], 1, x)[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26, 62, 75, 95, 105, 97, 120, 133, 151, 136]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [0 for i in range(0,10)]\n",
    "for i in range(0, 1000):\n",
    "    t1 = int(np.random.power(1.5, 1)[0]*10)\n",
    "    l[t1] += 1\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplearning import league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lel = league.League()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lel.buffer.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Play\n",
      "0  vs  2\n",
      "p1:  0.224 p2:  0.776 draw:  0.0\n",
      "1  vs  1\n",
      "p1:  0.483 p2:  0.505 draw:  0.012\n",
      "2  vs  0\n",
      "p1:  0.769 p2:  0.23 draw:  0.001\n",
      "2  vs  1\n",
      "p1:  0.704 p2:  0.292 draw:  0.004\n",
      "1  vs  2\n",
      "p1:  0.265 p2:  0.73 draw:  0.005\n",
      "2  vs  1\n",
      "p1:  0.698 p2:  0.298 draw:  0.004\n",
      "1  vs  1\n",
      "p1:  0.497 p2:  0.497 draw:  0.006\n",
      "2  vs  1\n",
      "p1:  0.699 p2:  0.297 draw:  0.004\n",
      "0  vs  2\n",
      "p1:  0.21 p2:  0.79 draw:  0.0\n",
      "2  vs  1\n",
      "p1:  0.714 p2:  0.28 draw:  0.006\n",
      "2  vs  0\n",
      "p1:  0.77 p2:  0.23 draw:  0.0\n",
      "1  vs  2\n",
      "p1:  0.317 p2:  0.681 draw:  0.002\n",
      "1  vs  0\n",
      "p1:  0.689 p2:  0.309 draw:  0.002\n",
      "0  vs  1\n",
      "p1:  0.329 p2:  0.67 draw:  0.001\n",
      "1  vs  2\n",
      "p1:  0.285 p2:  0.708 draw:  0.007\n",
      "0  vs  2\n",
      "p1:  0.225 p2:  0.775 draw:  0.0\n",
      "0  vs  1\n",
      "p1:  0.313 p2:  0.685 draw:  0.002\n",
      "1  vs  1\n",
      "p1:  0.483 p2:  0.502 draw:  0.015\n",
      "0  vs  0\n",
      "p1:  0.496 p2:  0.499 draw:  0.005\n",
      "1  vs  1\n",
      "p1:  0.497 p2:  0.491 draw:  0.012\n",
      "1  vs  1\n",
      "p1:  0.474 p2:  0.507 draw:  0.019\n",
      "2  vs  1\n",
      "p1:  0.706 p2:  0.293 draw:  0.001\n",
      "0  vs  1\n",
      "p1:  0.33 p2:  0.669 draw:  0.001\n",
      "1  vs  2\n",
      "p1:  0.289 p2:  0.706 draw:  0.005\n",
      "1  vs  0\n",
      "p1:  0.672 p2:  0.328 draw:  0.0\n",
      "Saving Agent\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_season\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\root\\projects\\UNI\\MasterWS22-23\\Deep Learning\\DL-Project\\deeplearning\\league.py:77\u001b[0m, in \u001b[0;36mLeague.play_season\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 4. Save Buffer + Save Agent\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseason))\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving Buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseason))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "lel.play_season()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-E6CLkrsR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
