{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import gym_2048\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:20: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the `dtype` is not `np.uint8`, actual type: int32. If the Box observation space is not an image, we recommend flattening the observation to have only a 1D vector.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('2048-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP UDPRL stolen from https://github.com/BY571/Upside-Down-Reinforcement-Learning/blob/master/Upside-Down.ipynb\n",
    "\n",
    "class BF(nn.Module):\n",
    "    def __init__(self, state_space, action_space, hidden_size, seed):\n",
    "        super(BF, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.actions = np.arange(action_space)\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(state_space, hidden_size)\n",
    "        self.commands = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc7 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc8 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc9 = nn.Linear(hidden_size, action_space)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, state, command):       \n",
    "               \n",
    "        out = self.sigmoid(self.fc1(state))\n",
    "        command_out = self.sigmoid(self.commands(command))\n",
    "        out = out * command_out\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = torch.relu(self.fc5(out))\n",
    "        out = torch.relu(self.fc6(out))\n",
    "        out = torch.relu(self.fc7(out))\n",
    "        out = torch.relu(self.fc8(out))\n",
    "        out = self.fc9(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Samples the action based on their probability\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire,horizon), dim=-1)\n",
    "        action_prob = self.forward(state.expand(1, -1), command.expand(1, -1))[0,:]\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.distributions.categorical.Categorical(probs=probs).sample()\n",
    "        return action\n",
    "    def greedy_action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Returns the greedy action \n",
    "        \"\"\"\n",
    "        command = torch.cat((desire,horizon), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def add_sample(self, states, actions, rewards):\n",
    "        episode = {\"states\": states, \"actions\":actions, \"rewards\": rewards, \"summed_rewards\":sum(rewards)}\n",
    "        self.buffer.append(episode)\n",
    "\n",
    "    def get_nbest(self, n):\n",
    "        self.sort()\n",
    "        return self.buffer[:n]\n",
    "        \n",
    "    \n",
    "    def sort(self):\n",
    "        #sort buffer\n",
    "        self.buffer = sorted(self.buffer, key = lambda i: i[\"summed_rewards\"],reverse=True)\n",
    "        # keep the max buffer size\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "    \n",
    "    def get_random_samples(self, batch_size):\n",
    "        self.sort()\n",
    "        idxs = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in idxs]\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bf = BF(256, 4, 2048, 1).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_desired_reward = 1\n",
    "init_time_horizon = 2\n",
    "\n",
    "def generate_episode(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):    \n",
    "    \"\"\"\n",
    "    Generates more samples for the replay buffer.\n",
    "    \"\"\"\n",
    "    next_state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = bf.action(torch.from_numpy(next_state.flatten()).float().to(device), desired_return, desired_time_horizon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        states.append(next_state.flatten())\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "    return states, actions, rewards, info['highest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\iguti\\.virtualenvs\\yannik_scribble-sRH0UPe6\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "100%|██████████| 1000/1000 [02:36<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create replay buffer:\n",
    "buffer = ReplayBuffer(replay_size)\n",
    "ma = 0\n",
    "# init replay buffer with random trajectories:\n",
    "for i in tqdm(range(0, int(replay_size/10))):\n",
    "    states, actions, rewards, h = generate_episode()\n",
    "    ma = max(h, ma)\n",
    "    buffer.add_sample(states, actions, rewards)\n",
    "print(\"Max: \", ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Sampling exploration commands\n",
    "last_few = 1000\n",
    "def sampling_exploration( top_X_eps = last_few):\n",
    "    \"\"\"\n",
    "    This function calculates the new desired reward and new desired horizon based on the replay buffer.\n",
    "    New desired horizon is calculted by the mean length of the best last X episodes. \n",
    "    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.\n",
    "    where X is the hyperparameter last_few.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    top_X = buffer.get_nbest(last_few)\n",
    "    #The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes\n",
    "    new_desired_horizon = np.mean([len(i[\"states\"]) for i in top_X])\n",
    "    # save all top_X cumulative returns in a list \n",
    "    returns = [i[\"summed_rewards\"] for i in top_X]\n",
    "    # from these returns calc the mean and std\n",
    "    mean_returns = np.mean(returns)\n",
    "    std_returns = np.std(returns)\n",
    "    # sample desired reward from a uniform distribution given the mean and the std\n",
    "    new_desired_reward = np.random.uniform(mean_returns, mean_returns+std_returns)\n",
    "\n",
    "    return torch.FloatTensor([new_desired_reward])  , torch.FloatTensor([new_desired_horizon]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# FUNCTIONS FOR TRAINING\n",
    "def select_time_steps(saved_episode):\n",
    "    \"\"\"\n",
    "    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:\n",
    "    T = max time horizon in that episode\n",
    "    Returns t1, t2 and T \n",
    "    \"\"\"\n",
    "    # Select times in the episode:\n",
    "    T = len(saved_episode[\"states\"]) # episode max horizon\n",
    "    if T != 1:\n",
    "        t1 = np.random.randint(0,T-1)\n",
    "        t2 = np.random.randint(t1+1,T)\n",
    "    else:\n",
    "        t1 = 0\n",
    "        t2 = T\n",
    "\n",
    "    return t1, t2, T\n",
    "\n",
    "def create_training_input(episode, to):\n",
    "    \"\"\"\n",
    "    Based on the selected episode and the given time steps this function returns 4 values:\n",
    "    1. state at t1\n",
    "    2. the desired reward: sum over all rewards from t1 to t2\n",
    "    3. the time horizont: t2 -t1\n",
    "    \n",
    "    4. the target action taken at t1\n",
    "    \n",
    "    buffer episodes are build like [cumulative episode reward, states, actions, rewards]\n",
    "    \"\"\"\n",
    "    state = episode[\"states\"][to] \n",
    "    rtg = sum(episode[\"rewards\"])\n",
    "    rtg = rtg - episode['rewards']\n",
    "    timesteps = t2-t1\n",
    "    action = episode[\"actions\"][t1]\n",
    "    return state, desired_reward, time_horizont, action\n",
    "\n",
    "def create_training_examples(batch_size):\n",
    "    \"\"\"\n",
    "    Creates a data set of training examples that can be used to create a data loader for training.\n",
    "    ============================================================\n",
    "    1. for the given batch_size episode idx are randomly selected\n",
    "    2. based on these episodes t1 and t2 are samples for each selected episode \n",
    "    3. for the selected episode and sampled t1 and t2 trainings values are gathered\n",
    "    ______________________________________________________________\n",
    "    Output are two numpy arrays in the length of batch size:\n",
    "    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)\n",
    "    Output Array with the taken actions \n",
    "    \"\"\"\n",
    "    input_array = []\n",
    "    output_array = []\n",
    "    # select randomly episodes from the buffer\n",
    "    episodes = buffer.get_random_samples(batch_size)\n",
    "    for ep in episodes:\n",
    "        #select time stamps\n",
    "        t1, t2, T = select_time_steps(ep)\n",
    "        # For episodic tasks they set t2 to T:\n",
    "        t2 = T\n",
    "        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)\n",
    "        input_array.append(torch.cat([torch.from_numpy(state).float(), torch.FloatTensor([desired_reward]), torch.FloatTensor([time_horizont])]))\n",
    "        output_array.append(action)\n",
    "    return input_array, output_array\n",
    "\n",
    "def train_behavior_function(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X, y = create_training_examples(batch_size)\n",
    "\n",
    "    X = torch.stack(X)\n",
    "    state = X[:,0:256]\n",
    "    d = X[:,256:256+1]\n",
    "    h = X[:,256+1:256+2]\n",
    "    command = torch.cat((d,h), dim=-1)\n",
    "    y = torch.stack(y).long()\n",
    "    y_ = bf(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    pred_loss = F.cross_entropy(y_, y)   \n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop():\n",
    "    i = 0\n",
    "    while True:\n",
    "        # compute desired horizon and desired reward\n",
    "        \n",
    "        i += 1\n",
    "        ma = 0\n",
    "        for _ in range(0, int(replay_size/10000)):\n",
    "            rew, hor = sampling_exploration()\n",
    "            states, actions, rewards, h = generate_episode(rew, hor)\n",
    "            ma = max(h,ma)\n",
    "            buffer.add_sample(states, actions, rewards)\n",
    "\n",
    "        loss = train_behavior_function(int(replay_size*10))\n",
    "        print(i, loss, \"Max: \", ma, \" RewardAim: \", rew, \" HorizonAim: \", hor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.3893855 Max:  16  RewardAim:  tensor([258.4325])  HorizonAim:  tensor([43.2080])\n",
      "2 1.3744199 Max:  32  RewardAim:  tensor([81.3916])  HorizonAim:  tensor([43.2080])\n",
      "3 1.3905137 Max:  4  RewardAim:  tensor([319.1218])  HorizonAim:  tensor([43.2110])\n",
      "4 1.3844566 Max:  16  RewardAim:  tensor([239.0560])  HorizonAim:  tensor([43.2110])\n",
      "5 1.3768514 Max:  4  RewardAim:  tensor([325.5722])  HorizonAim:  tensor([43.2110])\n",
      "6 1.377079 Max:  8  RewardAim:  tensor([285.9692])  HorizonAim:  tensor([43.2110])\n",
      "7 1.3841692 Max:  16  RewardAim:  tensor([186.6310])  HorizonAim:  tensor([43.2110])\n",
      "8 1.3865682 Max:  8  RewardAim:  tensor([324.3599])  HorizonAim:  tensor([43.2110])\n",
      "9 1.380036 Max:  16  RewardAim:  tensor([230.4178])  HorizonAim:  tensor([43.2110])\n",
      "10 1.3765268 Max:  4  RewardAim:  tensor([308.7067])  HorizonAim:  tensor([43.2110])\n",
      "11 1.3735746 Max:  4  RewardAim:  tensor([369.4398])  HorizonAim:  tensor([43.2110])\n",
      "12 1.3692938 Max:  8  RewardAim:  tensor([305.0653])  HorizonAim:  tensor([43.2110])\n",
      "13 1.3531786 Max:  8  RewardAim:  tensor([340.3506])  HorizonAim:  tensor([43.2110])\n",
      "14 1.3314445 Max:  4  RewardAim:  tensor([327.6422])  HorizonAim:  tensor([43.2110])\n",
      "15 1.2776462 Max:  8  RewardAim:  tensor([265.8235])  HorizonAim:  tensor([43.2110])\n",
      "16 1.2181376 Max:  4  RewardAim:  tensor([314.0845])  HorizonAim:  tensor([43.2110])\n",
      "17 1.3310483 Max:  16  RewardAim:  tensor([274.3389])  HorizonAim:  tensor([43.2110])\n",
      "18 1.1350244 Max:  16  RewardAim:  tensor([299.8582])  HorizonAim:  tensor([43.2110])\n",
      "19 2.3376343 Max:  8  RewardAim:  tensor([336.3432])  HorizonAim:  tensor([43.2110])\n",
      "20 1.2667265 Max:  8  RewardAim:  tensor([276.9467])  HorizonAim:  tensor([43.2110])\n",
      "21 1.4757454 Max:  4  RewardAim:  tensor([278.5245])  HorizonAim:  tensor([43.2110])\n",
      "22 1.4524759 Max:  8  RewardAim:  tensor([339.6313])  HorizonAim:  tensor([43.2110])\n",
      "23 1.4486456 Max:  2  RewardAim:  tensor([299.0872])  HorizonAim:  tensor([43.2110])\n",
      "24 1.4186158 Max:  8  RewardAim:  tensor([320.2994])  HorizonAim:  tensor([43.2110])\n",
      "25 1.4089984 Max:  32  RewardAim:  tensor([158.0038])  HorizonAim:  tensor([43.2110])\n",
      "26 1.4014015 Max:  16  RewardAim:  tensor([284.3028])  HorizonAim:  tensor([43.2110])\n",
      "27 1.3927051 Max:  8  RewardAim:  tensor([325.8227])  HorizonAim:  tensor([43.2110])\n",
      "28 1.3817766 Max:  16  RewardAim:  tensor([278.7912])  HorizonAim:  tensor([43.2110])\n",
      "29 1.3769563 Max:  8  RewardAim:  tensor([352.0792])  HorizonAim:  tensor([43.2110])\n",
      "30 1.384896 Max:  32  RewardAim:  tensor([159.4330])  HorizonAim:  tensor([43.2110])\n",
      "31 1.3671448 Max:  8  RewardAim:  tensor([318.0988])  HorizonAim:  tensor([43.2110])\n",
      "32 1.4220237 Max:  8  RewardAim:  tensor([341.5008])  HorizonAim:  tensor([43.2110])\n",
      "33 1.38885 Max:  4  RewardAim:  tensor([347.0894])  HorizonAim:  tensor([43.2110])\n",
      "34 1.3870821 Max:  32  RewardAim:  tensor([82.1156])  HorizonAim:  tensor([43.2110])\n",
      "35 1.3873937 Max:  64  RewardAim:  tensor([-91.3076])  HorizonAim:  tensor([43.2140])\n",
      "36 1.3875061 Max:  8  RewardAim:  tensor([276.4124])  HorizonAim:  tensor([43.2360])\n",
      "37 1.3868479 Max:  16  RewardAim:  tensor([190.9501])  HorizonAim:  tensor([43.2360])\n",
      "38 1.3862587 Max:  8  RewardAim:  tensor([248.6732])  HorizonAim:  tensor([43.2360])\n",
      "39 1.3871206 Max:  8  RewardAim:  tensor([336.5090])  HorizonAim:  tensor([43.2360])\n",
      "40 1.3872932 Max:  8  RewardAim:  tensor([351.6265])  HorizonAim:  tensor([43.2360])\n",
      "41 1.386051 Max:  2  RewardAim:  tensor([289.2675])  HorizonAim:  tensor([43.2360])\n",
      "42 1.3863574 Max:  32  RewardAim:  tensor([136.9094])  HorizonAim:  tensor([43.2360])\n",
      "43 1.3864409 Max:  8  RewardAim:  tensor([317.2213])  HorizonAim:  tensor([43.2390])\n",
      "44 1.3859304 Max:  4  RewardAim:  tensor([296.8052])  HorizonAim:  tensor([43.2390])\n",
      "45 1.3864218 Max:  4  RewardAim:  tensor([293.0938])  HorizonAim:  tensor([43.2390])\n",
      "46 1.3854302 Max:  16  RewardAim:  tensor([274.7620])  HorizonAim:  tensor([43.2390])\n",
      "47 1.3864152 Max:  4  RewardAim:  tensor([325.0103])  HorizonAim:  tensor([43.2390])\n",
      "48 1.3859762 Max:  16  RewardAim:  tensor([267.0834])  HorizonAim:  tensor([43.2390])\n",
      "49 1.3858075 Max:  2  RewardAim:  tensor([303.5454])  HorizonAim:  tensor([43.2390])\n",
      "50 1.3858429 Max:  16  RewardAim:  tensor([198.4366])  HorizonAim:  tensor([43.2390])\n",
      "51 1.387116 Max:  32  RewardAim:  tensor([156.6433])  HorizonAim:  tensor([43.2390])\n",
      "52 1.3859372 Max:  8  RewardAim:  tensor([285.5577])  HorizonAim:  tensor([43.2420])\n",
      "53 1.386866 Max:  16  RewardAim:  tensor([249.0331])  HorizonAim:  tensor([43.2420])\n",
      "54 1.3858242 Max:  8  RewardAim:  tensor([351.7803])  HorizonAim:  tensor([43.2420])\n",
      "55 1.3865403 Max:  8  RewardAim:  tensor([279.7188])  HorizonAim:  tensor([43.2420])\n",
      "56 1.3848187 Max:  16  RewardAim:  tensor([258.7618])  HorizonAim:  tensor([43.2420])\n",
      "57 1.384816 Max:  8  RewardAim:  tensor([278.3951])  HorizonAim:  tensor([43.2420])\n",
      "58 1.3840077 Max:  16  RewardAim:  tensor([229.1791])  HorizonAim:  tensor([43.2420])\n",
      "59 1.3825209 Max:  4  RewardAim:  tensor([279.9912])  HorizonAim:  tensor([43.2420])\n",
      "60 1.378033 Max:  32  RewardAim:  tensor([78.1449])  HorizonAim:  tensor([43.2420])\n",
      "61 1.3740441 Max:  16  RewardAim:  tensor([222.4410])  HorizonAim:  tensor([43.2560])\n",
      "62 1.364162 Max:  16  RewardAim:  tensor([228.0688])  HorizonAim:  tensor([43.2560])\n",
      "63 1.3714329 Max:  4  RewardAim:  tensor([318.0281])  HorizonAim:  tensor([43.2560])\n",
      "64 1.329096 Max:  2  RewardAim:  tensor([303.8601])  HorizonAim:  tensor([43.2560])\n",
      "65 1.2954323 Max:  16  RewardAim:  tensor([276.0559])  HorizonAim:  tensor([43.2560])\n",
      "66 1.3925716 Max:  16  RewardAim:  tensor([225.8105])  HorizonAim:  tensor([43.2560])\n",
      "67 1.389561 Max:  16  RewardAim:  tensor([269.2119])  HorizonAim:  tensor([43.2560])\n",
      "68 1.3946341 Max:  16  RewardAim:  tensor([152.9716])  HorizonAim:  tensor([43.2560])\n",
      "69 1.3877845 Max:  16  RewardAim:  tensor([246.8484])  HorizonAim:  tensor([43.2560])\n",
      "70 1.3882362 Max:  8  RewardAim:  tensor([350.3833])  HorizonAim:  tensor([43.2560])\n",
      "71 1.3859667 Max:  16  RewardAim:  tensor([252.7136])  HorizonAim:  tensor([43.2560])\n",
      "72 1.3860381 Max:  4  RewardAim:  tensor([317.0135])  HorizonAim:  tensor([43.2560])\n",
      "73 1.3858469 Max:  16  RewardAim:  tensor([265.8169])  HorizonAim:  tensor([43.2560])\n",
      "74 1.3862294 Max:  16  RewardAim:  tensor([222.5618])  HorizonAim:  tensor([43.2560])\n",
      "75 1.3868409 Max:  16  RewardAim:  tensor([254.8615])  HorizonAim:  tensor([43.2560])\n",
      "76 1.3885429 Max:  16  RewardAim:  tensor([224.6144])  HorizonAim:  tensor([43.2560])\n",
      "77 1.3863796 Max:  8  RewardAim:  tensor([325.0439])  HorizonAim:  tensor([43.2560])\n",
      "78 1.3861868 Max:  4  RewardAim:  tensor([357.9376])  HorizonAim:  tensor([43.2560])\n",
      "79 1.3855766 Max:  16  RewardAim:  tensor([208.0879])  HorizonAim:  tensor([43.2560])\n",
      "80 1.3850257 Max:  8  RewardAim:  tensor([309.1681])  HorizonAim:  tensor([43.2560])\n",
      "81 1.3852901 Max:  4  RewardAim:  tensor([306.1333])  HorizonAim:  tensor([43.2560])\n",
      "82 1.3859931 Max:  4  RewardAim:  tensor([294.8737])  HorizonAim:  tensor([43.2560])\n",
      "83 1.3835157 Max:  4  RewardAim:  tensor([285.9470])  HorizonAim:  tensor([43.2560])\n",
      "84 1.3815166 Max:  16  RewardAim:  tensor([221.9507])  HorizonAim:  tensor([43.2560])\n",
      "85 1.3754594 Max:  8  RewardAim:  tensor([285.9193])  HorizonAim:  tensor([43.2560])\n",
      "86 1.3657562 Max:  4  RewardAim:  tensor([346.8295])  HorizonAim:  tensor([43.2560])\n",
      "87 1.3502536 Max:  16  RewardAim:  tensor([231.1469])  HorizonAim:  tensor([43.2560])\n",
      "88 1.3923936 Max:  32  RewardAim:  tensor([148.9524])  HorizonAim:  tensor([43.2560])\n",
      "89 1.354653 Max:  8  RewardAim:  tensor([323.6608])  HorizonAim:  tensor([43.2560])\n",
      "90 1.3262655 Max:  2  RewardAim:  tensor([329.1995])  HorizonAim:  tensor([43.2560])\n",
      "91 1.2956507 Max:  4  RewardAim:  tensor([355.5963])  HorizonAim:  tensor([43.2560])\n",
      "92 1.2585043 Max:  16  RewardAim:  tensor([187.9470])  HorizonAim:  tensor([43.2560])\n",
      "93 1.2447718 Max:  2  RewardAim:  tensor([330.4809])  HorizonAim:  tensor([43.2560])\n",
      "94 1.7317208 Max:  4  RewardAim:  tensor([314.1599])  HorizonAim:  tensor([43.2560])\n",
      "95 1.3462299 Max:  32  RewardAim:  tensor([195.2857])  HorizonAim:  tensor([43.2560])\n",
      "96 1.448487 Max:  8  RewardAim:  tensor([282.4144])  HorizonAim:  tensor([43.2560])\n",
      "97 1.4229623 Max:  16  RewardAim:  tensor([214.6779])  HorizonAim:  tensor([43.2560])\n",
      "98 1.4076481 Max:  8  RewardAim:  tensor([355.5450])  HorizonAim:  tensor([43.2560])\n",
      "99 1.3751028 Max:  8  RewardAim:  tensor([284.5670])  HorizonAim:  tensor([43.2560])\n",
      "100 1.3564796 Max:  4  RewardAim:  tensor([283.1602])  HorizonAim:  tensor([43.2560])\n",
      "101 1.320655 Max:  4  RewardAim:  tensor([293.6491])  HorizonAim:  tensor([43.2560])\n",
      "102 1.2796904 Max:  8  RewardAim:  tensor([288.5080])  HorizonAim:  tensor([43.2560])\n",
      "103 1.2848722 Max:  4  RewardAim:  tensor([294.0121])  HorizonAim:  tensor([43.2560])\n",
      "104 1.2977548 Max:  32  RewardAim:  tensor([144.8406])  HorizonAim:  tensor([43.2560])\n",
      "105 1.2405274 Max:  32  RewardAim:  tensor([144.2940])  HorizonAim:  tensor([43.2560])\n",
      "106 1.1703454 Max:  8  RewardAim:  tensor([342.3331])  HorizonAim:  tensor([43.2560])\n",
      "107 1.1675515 Max:  16  RewardAim:  tensor([253.6479])  HorizonAim:  tensor([43.2560])\n",
      "108 1.0412809 Max:  4  RewardAim:  tensor([281.7924])  HorizonAim:  tensor([43.2560])\n",
      "109 1.0396 Max:  8  RewardAim:  tensor([265.7707])  HorizonAim:  tensor([43.2560])\n",
      "110 1.1812067 Max:  4  RewardAim:  tensor([313.4313])  HorizonAim:  tensor([43.2560])\n",
      "111 1.4131613 Max:  8  RewardAim:  tensor([343.3046])  HorizonAim:  tensor([43.2560])\n",
      "112 0.98776937 Max:  8  RewardAim:  tensor([313.6124])  HorizonAim:  tensor([43.2560])\n",
      "113 1.2538016 Max:  4  RewardAim:  tensor([293.7017])  HorizonAim:  tensor([43.2560])\n",
      "114 1.0184114 Max:  8  RewardAim:  tensor([312.6638])  HorizonAim:  tensor([43.2560])\n",
      "115 1.0562768 Max:  16  RewardAim:  tensor([303.5308])  HorizonAim:  tensor([43.2560])\n",
      "116 1.086289 Max:  4  RewardAim:  tensor([340.7179])  HorizonAim:  tensor([43.2560])\n",
      "117 1.076004 Max:  8  RewardAim:  tensor([328.0137])  HorizonAim:  tensor([43.2560])\n",
      "118 1.0374047 Max:  8  RewardAim:  tensor([293.2391])  HorizonAim:  tensor([43.2560])\n",
      "119 1.0542544 Max:  4  RewardAim:  tensor([316.8715])  HorizonAim:  tensor([43.2560])\n",
      "120 1.0638068 Max:  8  RewardAim:  tensor([257.6172])  HorizonAim:  tensor([43.2560])\n",
      "121 1.0239408 Max:  16  RewardAim:  tensor([242.9970])  HorizonAim:  tensor([43.2560])\n",
      "122 0.9812965 Max:  64  RewardAim:  tensor([-23.3289])  HorizonAim:  tensor([43.2560])\n",
      "123 0.984915 Max:  4  RewardAim:  tensor([286.4497])  HorizonAim:  tensor([43.2640])\n",
      "124 0.9719899 Max:  8  RewardAim:  tensor([270.3313])  HorizonAim:  tensor([43.2640])\n",
      "125 1.0297744 Max:  2  RewardAim:  tensor([330.1194])  HorizonAim:  tensor([43.2640])\n",
      "126 0.9591778 Max:  8  RewardAim:  tensor([302.9576])  HorizonAim:  tensor([43.2640])\n",
      "127 0.9805064 Max:  4  RewardAim:  tensor([297.7512])  HorizonAim:  tensor([43.2640])\n",
      "128 0.9700941 Max:  8  RewardAim:  tensor([308.7120])  HorizonAim:  tensor([43.2640])\n",
      "129 0.9954318 Max:  2  RewardAim:  tensor([337.1918])  HorizonAim:  tensor([43.2640])\n",
      "130 0.97884005 Max:  16  RewardAim:  tensor([305.2535])  HorizonAim:  tensor([43.2640])\n",
      "131 0.9738075 Max:  4  RewardAim:  tensor([321.5669])  HorizonAim:  tensor([43.2640])\n",
      "132 0.957685 Max:  4  RewardAim:  tensor([296.3963])  HorizonAim:  tensor([43.2640])\n",
      "133 0.9734496 Max:  8  RewardAim:  tensor([360.6113])  HorizonAim:  tensor([43.2640])\n",
      "134 0.937819 Max:  4  RewardAim:  tensor([331.0213])  HorizonAim:  tensor([43.2640])\n",
      "135 0.9532237 Max:  4  RewardAim:  tensor([292.7795])  HorizonAim:  tensor([43.2640])\n",
      "136 0.9463098 Max:  8  RewardAim:  tensor([302.0950])  HorizonAim:  tensor([43.2640])\n",
      "137 0.9424207 Max:  8  RewardAim:  tensor([346.1984])  HorizonAim:  tensor([43.2640])\n",
      "138 0.96186763 Max:  4  RewardAim:  tensor([325.2957])  HorizonAim:  tensor([43.2640])\n",
      "139 0.96469766 Max:  16  RewardAim:  tensor([270.5366])  HorizonAim:  tensor([43.2640])\n",
      "140 0.964545 Max:  32  RewardAim:  tensor([131.9526])  HorizonAim:  tensor([43.2640])\n",
      "141 0.97053164 Max:  2  RewardAim:  tensor([327.3473])  HorizonAim:  tensor([43.2640])\n",
      "142 0.9593346 Max:  2  RewardAim:  tensor([356.2985])  HorizonAim:  tensor([43.2640])\n",
      "143 0.9674182 Max:  8  RewardAim:  tensor([306.3618])  HorizonAim:  tensor([43.2640])\n",
      "144 0.95321923 Max:  8  RewardAim:  tensor([299.4222])  HorizonAim:  tensor([43.2640])\n",
      "145 0.9697201 Max:  8  RewardAim:  tensor([258.3854])  HorizonAim:  tensor([43.2640])\n",
      "146 0.97628236 Max:  8  RewardAim:  tensor([291.5352])  HorizonAim:  tensor([43.2640])\n",
      "147 0.9917512 Max:  4  RewardAim:  tensor([322.2771])  HorizonAim:  tensor([43.2640])\n",
      "148 0.9994436 Max:  16  RewardAim:  tensor([270.3715])  HorizonAim:  tensor([43.2640])\n",
      "149 0.95229626 Max:  8  RewardAim:  tensor([327.6389])  HorizonAim:  tensor([43.2640])\n",
      "150 0.95603335 Max:  2  RewardAim:  tensor([338.4383])  HorizonAim:  tensor([43.2640])\n",
      "151 0.9671201 Max:  8  RewardAim:  tensor([275.4025])  HorizonAim:  tensor([43.2640])\n",
      "152 0.9624372 Max:  8  RewardAim:  tensor([347.7384])  HorizonAim:  tensor([43.2640])\n",
      "153 0.98470265 Max:  16  RewardAim:  tensor([231.7836])  HorizonAim:  tensor([43.2640])\n",
      "154 0.9423513 Max:  16  RewardAim:  tensor([277.5074])  HorizonAim:  tensor([43.2640])\n",
      "155 0.95231545 Max:  16  RewardAim:  tensor([288.5182])  HorizonAim:  tensor([43.2640])\n",
      "156 0.9873433 Max:  4  RewardAim:  tensor([292.3441])  HorizonAim:  tensor([43.2640])\n",
      "157 0.9675873 Max:  16  RewardAim:  tensor([196.8616])  HorizonAim:  tensor([43.2640])\n",
      "158 0.97359765 Max:  16  RewardAim:  tensor([271.9979])  HorizonAim:  tensor([43.2640])\n",
      "159 0.92743146 Max:  4  RewardAim:  tensor([327.7103])  HorizonAim:  tensor([43.2640])\n",
      "160 0.9578615 Max:  4  RewardAim:  tensor([362.7260])  HorizonAim:  tensor([43.2640])\n",
      "161 0.9502634 Max:  8  RewardAim:  tensor([340.8943])  HorizonAim:  tensor([43.2640])\n",
      "162 0.9710854 Max:  32  RewardAim:  tensor([211.4413])  HorizonAim:  tensor([43.2640])\n",
      "163 0.9725209 Max:  16  RewardAim:  tensor([279.8003])  HorizonAim:  tensor([43.2640])\n",
      "164 0.9592719 Max:  4  RewardAim:  tensor([339.7654])  HorizonAim:  tensor([43.2640])\n",
      "165 0.9369656 Max:  4  RewardAim:  tensor([362.7030])  HorizonAim:  tensor([43.2640])\n",
      "166 0.9479541 Max:  8  RewardAim:  tensor([262.8863])  HorizonAim:  tensor([43.2640])\n",
      "167 0.9754716 Max:  4  RewardAim:  tensor([309.3785])  HorizonAim:  tensor([43.2640])\n",
      "168 0.95895654 Max:  8  RewardAim:  tensor([302.6141])  HorizonAim:  tensor([43.2640])\n",
      "169 0.9598483 Max:  16  RewardAim:  tensor([225.2916])  HorizonAim:  tensor([43.2640])\n",
      "170 0.9308532 Max:  16  RewardAim:  tensor([258.7772])  HorizonAim:  tensor([43.2640])\n",
      "171 0.9393471 Max:  4  RewardAim:  tensor([333.1199])  HorizonAim:  tensor([43.2640])\n",
      "172 0.9504251 Max:  2  RewardAim:  tensor([306.3720])  HorizonAim:  tensor([43.2640])\n",
      "173 0.95656955 Max:  4  RewardAim:  tensor([362.8796])  HorizonAim:  tensor([43.2640])\n"
     ]
    }
   ],
   "source": [
    "run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yannik_scribble-sRH0UPe6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
